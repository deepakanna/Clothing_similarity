{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369c5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1f8416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f4633a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44cb847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd25bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84cd239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7be540",
   "metadata": {},
   "source": [
    "# 1. Collect and Preprocess Data\n",
    "\n",
    "## Web Scraping tools used to gather dataset from online websites\n",
    "### web_scrap is a function to scrap data from the website passed as parameter \n",
    "### Beautiful Soup is the Web Scraping tool used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56808c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def web_scrap(url1,class1,split_string):\n",
    "    '''\n",
    "    INPUT:\n",
    "    url1: (String) - the URL of the website from which information is to be scraped\n",
    "    class1: (String) - the class information in the div tag in html found in the URL specified\n",
    "    split_string: (String) - the string is split using split_string\n",
    "    \n",
    "    OUTPUT:\n",
    "    d1 - (DataFrame) A DataFrame containing the data scraped from the website \n",
    "    \n",
    "    '''\n",
    "    list_web_plus=['https://www.pluss.in/men','https://www.pluss.in/women/skirts','https://www.pluss.in/women/denim','https://www.pluss.in/women/kurtis','https://www.pluss.in/women/formal-trouser','https://www.pluss.in/women/tracks-pants','https://www.pluss.in/women/tunics','https://www.pluss.in/women/tops','https://www.pluss.in/women/t-shirts','https://www.pluss.in/men/bermuda','https://www.pluss.in/men/t-shirt','https://www.pluss.in/men/kurta-pyjamas','https://www.pluss.in/kids/boys-t-shirt']\n",
    "\n",
    "    r=requests.get(url1)\n",
    "    #print(r.text)\n",
    "    soup=BeautifulSoup(r.content,'html.parser')\n",
    "    title=soup.title.getText()\n",
    "    #print(title)\n",
    "    #cloth=soup.find('div',{'class':'comp tax-sc__recirc-list card-list mntl-document-card-list mntl-card-list mntl-block'})\n",
    "    if(url1 in list_web_plus):\n",
    "        cloth=soup.find_all('div',{'class':class1})\n",
    "    else:\n",
    "        cloth=soup.find('div',{'class':class1})\n",
    "    #print(cloth)\n",
    "    #d1['item_name']=str(title)\n",
    "    list1=[]\n",
    "    for a in cloth:\n",
    "        if(split_string!=\"\"):\n",
    "            list1.append(a.getText().strip().split(split_string,2))\n",
    "        else:\n",
    "            list1.append(a.getText().strip())\n",
    "    #print(list1)\n",
    "    d1=pd.DataFrame(index=pd.Index(range(1,len(list1)+1)))\n",
    "\n",
    "    #print(len(list1))\n",
    "    #d1=pd.DataFrame(list1,columns=['index','Description'])\n",
    "    d1=pd.DataFrame(list1)\n",
    "    d1=d1.rename(columns={1:'Description'})  \n",
    "    d1['link']=url1\n",
    "    d1.insert(0,'item_name',title)\n",
    "    \n",
    "    #d1.drop(columns=[0],inplace=True)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94bafe",
   "metadata": {},
   "source": [
    "### The Websites from instore.co.in for the different clothing items are passed as a list to the function web_scrap specifying the class \n",
    "### value in the div tag found in the specified url and the split string used to split the items found in the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28054cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database():\n",
    "    list_web_sites=[\"https://instore.co.in/collections/leggings\",\"https://instore.co.in/collections/designer-dramas\",\"https://instore.co.in/collections/classics\",\"https://instore.co.in/collections/full-patiyalas\"]\n",
    "    d2=pd.DataFrame()\n",
    "    for i in list_web_sites:\n",
    "        dff=web_scrap(i,'grid grid--uniform grid--collection small--grid--flush','Quick view')\n",
    "        d2=pd.concat([d2,dff],axis=0)\n",
    "# The column 0 is removed from the DataFrame\n",
    "    d2.drop(columns=[0],inplace=True)\n",
    "    #d2\n",
    "    d3=web_scrap(\"https://www.instyle.com/fashion/clothing/shirts-and-tops\",'comp tax-sc__recirc-list card-list mntl-document-card-list mntl-card-list mntl-block','\\n')\n",
    "    d3=d3.rename(columns={0:'Description'})  \n",
    "    #d3\n",
    "    list_web_plus=['https://www.pluss.in/men','https://www.pluss.in/women/skirts','https://www.pluss.in/women/denim','https://www.pluss.in/women/kurtis','https://www.pluss.in/women/formal-trouser','https://www.pluss.in/women/tracks-pants','https://www.pluss.in/women/tunics','https://www.pluss.in/women/tops','https://www.pluss.in/women/t-shirts','https://www.pluss.in/men/bermuda','https://www.pluss.in/men/t-shirt','https://www.pluss.in/men/kurta-pyjamas','https://www.pluss.in/kids/boys-t-shirt']\n",
    "    d4=pd.DataFrame()\n",
    "    for j in list_web_plus:\n",
    "        dff=web_scrap(j,\"prodItem iprodItem\",\"\")\n",
    "        d4=pd.concat([d4,dff],axis=0)\n",
    "    d4=d4.rename(columns={0:'Description'})  \n",
    "    #d4\n",
    "    df=pd.concat([d2,d3,d4],axis=0)\n",
    "    df['sno']=range(1,df.shape[0]+1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8f534",
   "metadata": {},
   "source": [
    "## Preprocess the Text Data\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b11f142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis(df):\n",
    "    #print(\"The shape of the DataFrame is \",df.shape)\n",
    "    #print(\"The null values in the DataFrame are:\")\n",
    "    #print(df.isnull().sum())\n",
    "    #print(\"There may be some null strings in the Description which has to be replaced as nan\")\n",
    "    df['Description']=df['Description'].replace(\"\",np.nan)\n",
    "    #print(df['Description'].isnull().sum())\n",
    "    df=df.dropna()\n",
    "    #print(\"The Shape of the DataFrame after null values are removed:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a3e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    INPUT:\n",
    "    text: (String) - the Text which is to be tokenized\n",
    "    \n",
    "    OUTPUT:\n",
    "    clean_tokens - (String) Clean Tokens \n",
    "    \n",
    "    Process:\n",
    "    The text is converted to lower case and any special characters are removed. The sentence is then converted to tokens \n",
    "    and stopwords are removed. WordNetlemmatizer is applied to lemmatize and clean tokens are obtained.\n",
    "    '''\n",
    "    #col=df_content['doc_full_name']\n",
    "    #print(col)\n",
    "    clean_tokens=[]\n",
    "    #for ln in text:\n",
    "        #print(ln)\n",
    "    sent=str(text)\n",
    "    sent=sent.lower()\n",
    "    sent1=re.sub('[0-9]','',sent)\n",
    "    sp=re.compile('<.*?(#-,)>')\n",
    "    sent2=re.sub(sp,'',sent1)\n",
    "    tokens=word_tokenize(sent2)\n",
    "    words=[w for w in tokens if len(w)>3 if w not in stopwords.words('english')]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    for tok in words:\n",
    "        cl=lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(cl)\n",
    "    clean_tokens=' '.join(clean_tokens)  # s=set(clean_tokens)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19d9ab",
   "metadata": {},
   "source": [
    "# 2. Measure Similarity and retrieve Ranked Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "023926aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rank(df,inp_text):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df: (DataFrame) - the DataFrame containing the scraped data from websites\n",
    "    inp_text: (String) -  The input string for which similar url's are to be determined\n",
    "    \n",
    "    OUTPUT:\n",
    "    suggested_urls - json - A json object of suggested URL's that are similar to the input text\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    input_cl=tokenize(inp_text)\n",
    "    #print(input_cl)\n",
    "    cv=CountVectorizer()\n",
    "    word_count=cv.fit_transform(df['Description'])\n",
    "    #print(word_count.shape)\n",
    "    cv.fit(df['Description'])\n",
    "    inp_vect=cv.transform([input_cl])\n",
    "    ds_vect=cv.transform(df['Description'])\n",
    "    sim1=cosine_similarity(inp_vect,ds_vect)\n",
    "    sort_ind=sim1.argsort()[0][::-1]\n",
    "    #print(sort_ind)\n",
    "    sugg=[]\n",
    "    for i in sort_ind[:15]:\n",
    "        u=df[df['sno']==i].link.values[0]\n",
    "        if u not in sugg:\n",
    "            sugg.append(u)\n",
    "    suggested={\n",
    "        'suggestions':sugg\n",
    "    }\n",
    "    suggested_urls=json.dumps(suggested)\n",
    "    #sg=sorted(set(map(tuple,sugg)),reverse=True)\n",
    "    return suggested_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c825c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clothing_similarity_search(input_text):\n",
    "    list_web_plus=['https://www.pluss.in/men','https://www.pluss.in/women/skirts','https://www.pluss.in/women/denim','https://www.pluss.in/women/kurtis','https://www.pluss.in/women/formal-trouser','https://www.pluss.in/women/tracks-pants','https://www.pluss.in/women/tunics','https://www.pluss.in/women/tops','https://www.pluss.in/women/t-shirts','https://www.pluss.in/men/bermuda','https://www.pluss.in/men/t-shirt','https://www.pluss.in/men/kurta-pyjamas','https://www.pluss.in/kids/boys-t-shirt']\n",
    "    \n",
    "    df_final=create_database()\n",
    "    data_analysis(df_final)\n",
    "    df_final['Description']=df_final['Description'].apply(tokenize)\n",
    "    urls=similarity_rank(df_final,input_text)\n",
    "    print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20e880e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"suggestions\": [\"https://www.pluss.in/kids/boys-t-shirt\", \"https://www.pluss.in/men/kurta-pyjamas\", \"https://www.pluss.in/women/tops\", \"https://www.pluss.in/men/t-shirt\", \"https://www.pluss.in/men\", \"https://www.pluss.in/women/t-shirts\"]}\n"
     ]
    }
   ],
   "source": [
    "Clothing_similarity_search(\"This is a stylish and comfortable t-shirt in black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140df20a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
