{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a20c465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the required libraries\n",
    "import pandas as pd \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e9b6fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02cf2d",
   "metadata": {},
   "source": [
    "## Preprocess the Text Data\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "297db98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis(df):\n",
    "    print(\"The shape of the DataFrame is \",df.shape)\n",
    "    print(\"The null values in the DataFrame are:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"There may be some null strings in the Description which has to be replaced as nan\")\n",
    "    df['Description']=df['Description'].replace(\"\",np.nan)\n",
    "    print(df['Description'].isnull().sum())\n",
    "    df=df.dropna()\n",
    "    print(\"The Shape of the DataFrame after null values are removed:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6be8c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    INPUT:\n",
    "    text: (String) - the Text which is to be tokenized\n",
    "    \n",
    "    OUTPUT:\n",
    "    clean_tokens - (String) Clean Tokens \n",
    "    \n",
    "    Process:\n",
    "    The text is converted to lower case and any special characters are removed. The sentence is then converted to tokens \n",
    "    and stopwords are removed. WordNetlemmatizer is applied to lemmatize and clean tokens are obtained.\n",
    "    '''\n",
    "        \n",
    "    clean_tokens=[]\n",
    "    sent=str(text)\n",
    "    sent=sent.lower()\n",
    "    #sent1=re.sub('[0-9]','',sent)\n",
    "    sp=re.compile('<.*?(#,-)>')\n",
    "    sent2=re.sub(sp,'',sent)\n",
    "    \n",
    "    # Sentences are converted to tokens and stopwords removed\n",
    "    tokens=word_tokenize(sent2)\n",
    "    words=[w for w in tokens if len(w)>=1 if w not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    for tok in words:\n",
    "        cl=lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(cl)\n",
    "    clean_tokens=' '.join(clean_tokens)  \n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5f526",
   "metadata": {},
   "source": [
    "# 2. Measure Similarity and retrieve Ranked Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ea2850e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rank(df,inp_text):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df: (DataFrame) - the DataFrame containing the scraped data from websites\n",
    "    inp_text: (String) -  The input string for which similar url's are to be determined\n",
    "    \n",
    "    OUTPUT:\n",
    "    suggested_urls - json - A json object of suggested URL's that are similar to the input text\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #The input text is converted to tokens\n",
    "    input_cl=tokenize(inp_text)\n",
    "    \n",
    "    #print(input_cl)\n",
    "    cv=CountVectorizer()\n",
    "    \n",
    "    \n",
    "    # Count Vectorizer converts the data into numerical values. Both the input data and the dataset Description attribute are converted\n",
    "    cv.fit(df['Description'])\n",
    "    inp_vect=cv.transform([input_cl])\n",
    "    ds_vect=cv.transform(df['Description'])\n",
    "    #print(ds_vect)\n",
    "\n",
    "    # The cosine cimilarity between the input vector and the dataset Description vector is computed to find the similarity between the two text\n",
    "    sim1=cosine_similarity(inp_vect,ds_vect)\n",
    "    \n",
    "    # The similarity is sorted in the reverse order to obtain the most similar top elements\n",
    "    sort_ind=sim1.argsort()[0][::-1]\n",
    "    #print(sort_ind[0:20])\n",
    "    \n",
    "    sugg1=[]\n",
    "    \n",
    "    for i in sort_ind:\n",
    "        i+=1\n",
    "        u=df[df['sno']==i]['link'].values[0]\n",
    "        if u not in sugg1:\n",
    "            sugg1.append(u)\n",
    "        \n",
    "    # The top 10 suggested urls are insertd in a dictionary\n",
    "    sugg=sugg1[0:10]\n",
    "    suggested={\n",
    "        'suggestions':sugg\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # The dictionary is converted to json file\n",
    "    suggested_urls=json.dumps(suggested,indent=1)\n",
    "    \n",
    "    # The suggested urls are returned\n",
    "    return suggested_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "598666f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clothing_similarity_search(input_text):\n",
    "    \n",
    "    # The scraped data is stored in webscrap_data.csv file. This is loaded in df_final  \n",
    "    df_final=pd.read_csv('webscrap_data.csv')\n",
    "   \n",
    "    # The dataset is analysed using data_analysis function\n",
    "    data_analysis(df_final)\n",
    "    \n",
    "    # The tokenize function is applied to the Description attribute of the dataset\n",
    "    df_final['Description']=df_final['Description'].apply(tokenize)\n",
    "\n",
    "    # The similarity between input_text and the dataset is determined by calling the similarity_rank function\n",
    "    urls=similarity_rank(df_final,input_text)\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d91d6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the DataFrame is  (1003, 3)\n",
      "The null values in the DataFrame are:\n",
      "Description    0\n",
      "link           0\n",
      "sno            0\n",
      "dtype: int64\n",
      "There may be some null strings in the Description which has to be replaced as nan\n",
      "0\n",
      "The Shape of the DataFrame after null values are removed: (1003, 3)\n"
     ]
    }
   ],
   "source": [
    "suggested_URLs=Clothing_similarity_search(\"This is a stylish and comfortable pant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5476eb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"suggestions\": [\n",
      "  \"https://www.pluss.in/women/black-track-pant-lpj8787-black.html\",\n",
      "  \"https://instore.co.in//collections/offers/products/thumbi-thullal-kurti-coat-pant\",\n",
      "  \"https://www.pluss.in/women/navy-blue-track-pant-lpj8787-navy.html\",\n",
      "  \"https://instore.co.in//collections/offers/products/shireen\",\n",
      "  \"https://www.pluss.in/men/men-black-solid-straight-fit-track-pants-mpjs12324-black.html\",\n",
      "  \"https://www.pluss.in/men/men-black-solid-straight-fit-track-pants-mpjs11106-black.html\",\n",
      "  \"https://www.pluss.in/men/men-navy-blue-solid-straight-fit-cotton-track-pants-mpjs12323-navy.html\",\n",
      "  \"https://instore.co.in//collections/offers/products/janet\",\n",
      "  \"https://www.pluss.in/women/women-off-white-solid-straight-palazzos-ldr5536-off-white.html\",\n",
      "  \"https://www.pluss.in/women/navy-churidar-leggings-llg3199-navy.html\"\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(suggested_URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "41fcfd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the DataFrame is  (1003, 3)\n",
      "The null values in the DataFrame are:\n",
      "Description    0\n",
      "link           0\n",
      "sno            0\n",
      "dtype: int64\n",
      "There may be some null strings in the Description which has to be replaced as nan\n",
      "0\n",
      "The Shape of the DataFrame after null values are removed: (1003, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n \"suggestions\": [\\n  \"https://instore.co.in//collections/offers/products/manohariblue\",\\n  \"https://instore.co.in//collections/offers/products/blue-lily\",\\n  \"https://instore.co.in//collections/offers/products/kashmira-pastel-peach\",\\n  \"https://www.pluss.in/women/navy-blue-track-pant-lpj8787-navy.html\",\\n  \"https://www.pluss.in/men/men-navy-blue-solid-regular-shorts-mbr9483-navy.html\",\\n  \"https://www.pluss.in/women/women-blue-white-printed-straight-kurta-llkt7268-blue-print.html\",\\n  \"https://www.pluss.in/women/women-blue-wide-leg-solid-palazzos-lpzo6732-bata-wash.html\",\\n  \"https://www.pluss.in/women/women-blue-wide-leg-solid-palazzos-lpzo6732-enzyme-wash.html\",\\n  \"https://www.pluss.in/women/women-blue-solid-regular-fit-capris-lcp7812-royal-blue.html\",\\n  \"https://instore.co.in//collections/offers/products/aarathya-blue-kurti-dupatta\"\\n ]\\n}'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clothing_similarity_search(\"This is a blue dress\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
